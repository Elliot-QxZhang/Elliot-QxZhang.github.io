---
title:          "Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration"
date:           2025-5-1 00:01:00 +0800
selected:       true
pub:            "Under Review"
pub_last:       ' <span class="badge badge-pill badge-secondary badge-publication">MLLM</span>'
pub_date:       "2025"
abstract: >-
 Generalist Medical AI systems have demonstrated expert-level performance in biomedical perception tasks, yet their clinical utility remains limited by inadequate multi-modal explainability and suboptimal prognostic capabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI assistant that integrates textual and visual interpretability to support transparent and trustworthy medical decision-making. XMedGPT not only produces accurate diagnostic and descriptive outputs, but also grounds referenced anatomical sites within medical images, bridging critical gaps in interpretability and enhancing clinician usability. The model achieves an Intersection over Union of 0.703 across 141 anatomical regions, and a Kendallâ€™s tau-b of 0.479, demonstrating strong alignment between visual rationales and clinical outcomes. In survival and recurrence prediction, it surpasses prior leading models by 26.9%...
cover: /assets/images/covers/xmedgpt.png
authors:
  - Honglong Yang
  - Shanshan Song
  - Yi Qin
  - Lehan Wang
  - Haonan Wang
  - Xinpeng Ding
  - <b>Qixiang Zhang</b>
  - Bodong Du
  - Xiaomeng Li
links:
  Preprint: https://arxiv.org/abs/2505.06898
---

