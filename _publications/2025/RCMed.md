---
title:          "Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant"
date:           2025-5-1 00:01:00 +0800
selected:       true
pub:            "Nature Communication"
pub_pre:        "Submitted to "
pub_post:       ' Under Review.'
pub_last:       ' <span class="badge badge-pill badge-success badge-publication">Segmentation</span>'
pub_date:       "2025"
abstract: >-
 In this paper, we propose RCMed, a full-stack AI assistant that enhances multimodal alignment in both input and output, enabling precise anatomical delineation, accurate localization, and reliable diagnosis for clinicians through hierarchical vision-language grounding. We establish a self-reinforcing correlation mechanism where visual features dynamically inform language context, while language semantics guide pixel-wise spatial attention, creating a closed-loop system that progressively refines both modalities. The strong correlation is enhanced by a color region description strategy, which translates anatomical structures into semantically rich textual descriptors, enabling the model to learn intrinsic shape-location-text relationships across scales. Trained on a 20 million images-mask-description triplets dataset, RCMed achieves SOTA precision in contextualizing irregular lesions and subtle anatomical boundaries, excelling across 165 clinical tasks with 9 different modalities.
cover: /assets/images/covers/rc_med.png
authors:
  - Haonan Wang
  - Jiaji Mao
  - Lehan Wang
  - <b>Qixiang Zhang</b>
  - Marawan Elbatel
  - Yi Qin
  - Huijun Hu
  - Baoxun Li
  - Wenhui Deng
  - Weifeng Qin
  - Hongrui Li
  - Jialin Liang
  - Jun Shen
  - Xiaomeng Li
links:
  Preprint: https://arxiv.org/abs/2505.03380
  Code: None
---
